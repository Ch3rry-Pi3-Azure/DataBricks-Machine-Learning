{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba9206b8-263c-4c9e-86c0-9caf6b565f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### <span style=\"color:#1f77b4\">**Unity Catalog configuration**</span>\n",
    "\n",
    "Set up widgets for `CATALOG`, `SCHEMA`, and `VOLUME`, resolve the active catalog, and build the `BASE` path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a086541f-777f-4f4c-8803-aa7fcf325436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Unity Catalog widgets and resolve the active catalog.\n",
    "\n",
    "# Unity Catalog config for this project\n",
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"CATALOG\", \"\")\n",
    "dbutils.widgets.text(\"SCHEMA\", \"default\")\n",
    "dbutils.widgets.text(\"VOLUME\", \"ml_lab\")\n",
    "\n",
    "catalog_widget = dbutils.widgets.get(\"CATALOG\")\n",
    "if catalog_widget:\n",
    "    CATALOG = catalog_widget\n",
    "else:\n",
    "    # Prefer current catalog, otherwise pick the first non-system catalog\n",
    "    current = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
    "    catalogs = [r.catalog for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    CATALOG = current if current not in (\"system\",) else next(c for c in catalogs if c not in (\"system\",))\n",
    "\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "VOLUME = dbutils.widgets.get(\"VOLUME\")\n",
    "BASE = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f8d657e-c431-45ba-9ee1-d8f4759e2765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### <span style=\"color:#1f77b4\">**Create schema and volume**</span>\n",
    "\n",
    "Ensure the Unity Catalog schema and volume exist before loading data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e624def8-5ed3-4708-ac64-5ce619804233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the schema and volume if needed.\n",
    "\n",
    "# Ensure schema and volume exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8778fd1-a352-40ce-911f-04af4250bf28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### <span style=\"color:#1f77b4\">**Load data into the UC volume**</span>\n",
    "\n",
    "Copy the diabetes CSV into the Unity Catalog volume only if it is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f3e5f4-354f-483b-bb96-5a812618701d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy the dataset only if it is missing.\n",
    "\n",
    "# Sync raw data files into the UC volume (only if missing)\n",
    "data_dir = f\"{BASE}/diabetes\"\n",
    "data_file = f\"{data_dir}/diabetes.csv\"\n",
    "try:\n",
    "    dbutils.fs.ls(data_file)\n",
    "    file_exists = True\n",
    "except Exception:\n",
    "    file_exists = False\n",
    "\n",
    "if not file_exists:\n",
    "    dbutils.fs.mkdirs(data_dir)\n",
    "    dbutils.fs.cp(\"https://raw.githubusercontent.com/Ch3rry-Pi3-Azure/DataBricks-Machine-Learning/refs/heads/main/data/diabetes.csv\", data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6bafe4a-2d7d-4416-a3f3-708e741043b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### <span style=\"color:#1f77b4\">**Load, clean, and cache data**</span>\n",
    "\n",
    "Read the CSV, cast types, and cache the dataset and splits to avoid inconsistent reads during tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62214acb-1612-4112-bd7a-b67399fede6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows: 545  Testing Rows: 223\n"
     ]
    }
   ],
   "source": [
    "# Cast columns, split, and cache to stabilize reads.\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "   \n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(BASE + \"/diabetes/diabetes.csv\")\n",
    "data = data.dropna().select(col(\"Pregnancies\").astype(\"int\"),\n",
    "                           col(\"Glucose\").astype(\"int\"),\n",
    "                          col(\"BloodPressure\").astype(\"int\"),\n",
    "                          col(\"SkinThickness\").astype(\"int\"),\n",
    "                          col(\"Insulin\").astype(\"int\"),\n",
    "                          col(\"BMI\").astype(\"float\"),\n",
    "                          col(\"DiabetesPedigreeFunction\").astype(\"float\"),\n",
    "                          col(\"Age\").astype(\"int\"),\n",
    "                          col(\"Outcome\").astype(\"int\")\n",
    "                          )\n",
    "\n",
    "   \n",
    "data = data.cache()\n",
    "_ = data.count()\n",
    "\n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "train = train.cache()\n",
    "test = test.cache()\n",
    "_ = train.count()\n",
    "_ = test.count()\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76b61bdf-483d-443e-8527-359a90b6a50b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### <span style=\"color:#1f77b4\">**Tune with TrainValidationSplit**</span>\n",
    "\n",
    "Use Spark ML?s built?in `TrainValidationSplit` to evaluate a small parameter grid without extra dependencies. This is faster than full cross?validation and works well on single?node clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28571957-a327-4422-830d-7da5057f2dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c56ab3cf3724ccea558a402049231ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39265c638e643a1b5b40ff110711cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba502c0e50174fd4a2a946fcac79453f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d26f19d953486a93b03d9bde703c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model accuracy: 0.6636771300448431\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Assemble features and define the model\n",
    "numFeatures = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "numScaler = MinMaxScaler(inputCol=numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "featureVector = VectorAssembler(inputCols=[\"normalizedFeatures\"], outputCol=\"Features\")\n",
    "mlAlgo = DecisionTreeClassifier(labelCol=\"Outcome\", featuresCol=\"Features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[numVector, numScaler, featureVector, mlAlgo])\n",
    "\n",
    "# Define a small hyperparameter grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(mlAlgo.maxDepth, [2, 4, 6, 8])\n",
    "    .addGrid(mlAlgo.maxBins, [10, 20, 30])\n",
    "    .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Use TrainValidationSplit for faster tuning on single-node compute\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "tvs_model = tvs.fit(train)\n",
    "best_model = tvs_model.bestModel\n",
    "\n",
    "# Evaluate the best model on the held-out test set\n",
    "pred = best_model.transform(test)\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "print(f\"Best model accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2aad99-5ecb-4608-83ed-1c782018d3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### <span style=\"color:#1f77b4\">**Register the tuned model**</span>\n",
    "\n",
    "Log the best model from tuning to MLflow with a signature and register it in Unity Catalog so it appears in the Models tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8912382-2f6d-4fe0-9c4f-1b55ea2d486f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/mlflow/types/utils.py:406: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  warnings.warn(\n2025/12/28 16:45:34 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a856105bc7c542889d8114d08b6220af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/28 16:46:00 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/2636880519013444/e8323df8d9f749e4b19d5388281c9f53/artifacts/model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954faafd33a2493089c6595ae6194ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'dbw_databricks_ml_jaguar.default.diabetes_tree_tuned'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5adcf4643944c4be1c138577d32a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ede97a841274424a3c6465547f83239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'dbw_databricks_ml_jaguar.default.diabetes_tree_tuned'.\n2025/12/28 16:46:07 INFO mlflow.tracking._tracking_service.client: \uD83C\uDFC3 View run marvelous-sloth-134 at: adb-7405608564792326.6.azuredatabricks.net/ml/experiments/2636880519013444/runs/e8323df8d9f749e4b19d5388281c9f53.\n2025/12/28 16:46:07 INFO mlflow.tracking._tracking_service.client: \uD83E\uDDEA View experiment at: adb-7405608564792326.6.azuredatabricks.net/ml/experiments/2636880519013444.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered dbw_databricks_ml_jaguar.default.diabetes_tree_tuned from run e8323df8d9f749e4b19d5388281c9f53\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Use UC volume for MLflow temp staging\n",
    "mlflow_tmp = f\"{BASE}/mlflow_tmp\"\n",
    "dbutils.fs.mkdirs(mlflow_tmp)\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = mlflow_tmp\n",
    "\n",
    "# Prepare input/output samples for signature\n",
    "feature_cols = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "input_df = train.select(*feature_cols).limit(20)\n",
    "output_df = best_model.transform(input_df).select(\"prediction\").limit(20)\n",
    "signature = infer_signature(input_df, output_df)\n",
    "input_example = input_df.limit(5).toPandas()\n",
    "\n",
    "# Register the tuned model\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{CATALOG}.{SCHEMA}.diabetes_tree_tuned\"\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=best_model,\n",
    "        artifact_path=\"model\",\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "    print(f\"Registered {model_name} from run {run.info.run_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_hyperopt.ipynb",
   "widgets": {
    "CATALOG": {
     "currentValue": "",
     "nuid": "774c8892-48e5-4c89-9f1c-f5f4d8269179",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "CATALOG",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "CATALOG",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "SCHEMA": {
     "currentValue": "default",
     "nuid": "b63e80c2-9db3-4c05-98db-d0ae68fc3136",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "SCHEMA",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": null,
      "name": "SCHEMA",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "VOLUME": {
     "currentValue": "ml_lab",
     "nuid": "bf87181a-dd3d-4562-811c-e1bd07508880",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ml_lab",
      "label": null,
      "name": "VOLUME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ml_lab",
      "label": null,
      "name": "VOLUME",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}