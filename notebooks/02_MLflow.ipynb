{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#1f77b4\">**Machine Learning 02 - MLflow**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Catalog storage setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Unity Catalog config for this project\n",
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"CATALOG\", \"\")\n",
    "dbutils.widgets.text(\"SCHEMA\", \"default\")\n",
    "dbutils.widgets.text(\"VOLUME\", \"ml_lab\")\n",
    "\n",
    "catalog_widget = dbutils.widgets.get(\"CATALOG\")\n",
    "if catalog_widget:\n",
    "    CATALOG = catalog_widget\n",
    "else:\n",
    "    # Prefer current catalog, otherwise pick the first non-system catalog\n",
    "    current = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
    "    catalogs = [r.catalog for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    CATALOG = current if current not in (\"system\",) else next(c for c in catalogs if c not in (\"system\",))\n",
    "\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "VOLUME = dbutils.widgets.get(\"VOLUME\")\n",
    "BASE = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ensure schema and volume exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Configuring MLflow temp storage**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use UC volume for MLflow temp artifacts to avoid DBFS root\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "base_local = \"/dbfs\" + BASE.replace(\"dbfs:\", \"\")\n",
    "mlflow_tmp = base_local + \"/mlflow_tmp\"\n",
    "dbutils.fs.mkdirs(BASE + \"/mlflow_tmp\")\n",
    "os.environ[\"MLFLOW_TMP_DIR\"] = mlflow_tmp\n",
    "\n",
    "# Store MLflow artifacts in the UC volume\n",
    "mlflow_artifacts = BASE + \"/mlflow_artifacts\"\n",
    "experiment_name = \"/Shared/machine-learning/diabetes-mlflow\"\n",
    "try:\n",
    "    exp_id = mlflow.create_experiment(experiment_name, artifact_location=mlflow_artifacts)\n",
    "except Exception:\n",
    "    exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "    exp_id = exp.experiment_id if exp else None\n",
    "if exp_id:\n",
    "    mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Loading CSV Dataset into the Databricks File System (DBFS)**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sync raw data files into the UC volume\n",
    "data_dir = f\"{BASE}/diabetes\"\n",
    "dbutils.fs.rm(data_dir, recurse=True)\n",
    "dbutils.fs.mkdirs(data_dir)\n",
    "dbutils.fs.cp(\"https://raw.githubusercontent.com/Ch3rry-Pi3-Azure/DataBricks-Machine-Learning/refs/heads/main/data/diabetes.csv\", f\"{BASE}/diabetes/diabetes.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Splitting Dataset into Training and Testing sets**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "   \n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(BASE + \"/diabetes/diabetes.csv\")\n",
    "data = data.dropna().select(col(\"Pregnancies\").astype(\"int\"),\n",
    "                           col(\"Glucose\").astype(\"int\"),\n",
    "                          col(\"BloodPressure\").astype(\"int\"),\n",
    "                          col(\"SkinThickness\").astype(\"int\"),\n",
    "                          col(\"Insulin\").astype(\"int\"),\n",
    "                          col(\"BMI\").astype(\"float\"),\n",
    "                          col(\"DiabetesPedigreeFunction\").astype(\"float\"),\n",
    "                          col(\"Age\").astype(\"int\"),\n",
    "                          col(\"Outcome\").astype(\"int\")\n",
    "                          )\n",
    "\n",
    "   \n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Creating an MLflow Experiment Function**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_diabetes_model(training_data, test_data, maxIterations, regularization):\n",
    "    import mlflow\n",
    "    import mlflow.spark\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    import time\n",
    "    \n",
    "    # Start an MLflow run  \n",
    "    with mlflow.start_run():\n",
    "        numFeatures = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "\n",
    "        # define feature engineering and model steps\n",
    "\n",
    "        numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "        numScaler = MinMaxScaler(inputCol=numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "        featureVector = VectorAssembler(inputCols=[\"normalizedFeatures\"], outputCol=\"features\")\n",
    "        algo = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"features\", maxIter=maxIterations, regParam=regularization)\n",
    "\n",
    "        # chain the steps as stages in a Pipeline\n",
    "        Pipeline = Pipeline(stages=[numVector, numScaler,featureVector,algo])\n",
    "\n",
    "        # Log training parameter values\n",
    "        print (\"Training Logistic Regression model...\")\n",
    "        mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "        mlflow.log_param('regParam', algo.getRegParam())\n",
    "        model = Pipeline.fit(training_data)\n",
    "   \n",
    "        # Evaluate the model and log metrics\n",
    "        prediction = model.transform(test_data)\n",
    "        metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "        for metric in metrics:\n",
    "            evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=metric)\n",
    "            metricValue = evaluator.evaluate(prediction)\n",
    "            print(\"%s: %s\" % (metric, metricValue))\n",
    "            mlflow.log_metric(metric, metricValue)\n",
    "   \n",
    "   \n",
    "        # Log the model itself\n",
    "        unique_model_name = \"classifier-\" + str(time.time())\n",
    "        mlflow.spark.log_model(model, unique_model_name, mlflow.spark.get_default_conda_env())\n",
    "        modelpath = BASE + \"/models/%s\" % (unique_model_name)\n",
    "        mlflow.spark.save_model(model, modelpath)\n",
    "   \n",
    "        print(\"Experiment run complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Calling our MLflow experiment function with different hyperparameters**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_diabetes_model(train, test, 5, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_diabetes_model(train, test, 10, 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Testing our registered model via browser based endpoint**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "   \"dataframe_records\": [\n",
    "   {\n",
    "      \"Pregnancies\": 8,\n",
    "      \"Glucose\": 85,\n",
    "      \"BloodPressure\": 65,\n",
    "      \"SkinThickness\": 29,\n",
    "      \"Insulin\": 0,\n",
    "      \"BMI\": 26.6,\n",
    "      \"DiabetesPedigreeFunction\": 0.672,\n",
    "      \"Age\": 34\n",
    "   }\n",
    "   ]\n",
    " }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}