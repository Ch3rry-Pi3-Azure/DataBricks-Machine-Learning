{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#1f77b4\">**Machine Learning 02 - MLflow**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Unity Catalog configuration**</span>\n",
    "\n",
    "Set up widgets for `CATALOG`, `SCHEMA`, and `VOLUME`, resolve the active catalog, and build the `BASE` path.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configure Unity Catalog widgets and resolve the active catalog.\n",
    "\n",
    "# Unity Catalog config for this project\n",
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"CATALOG\", \"\")\n",
    "dbutils.widgets.text(\"SCHEMA\", \"default\")\n",
    "dbutils.widgets.text(\"VOLUME\", \"ml_lab\")\n",
    "\n",
    "catalog_widget = dbutils.widgets.get(\"CATALOG\")\n",
    "if catalog_widget:\n",
    "    CATALOG = catalog_widget\n",
    "else:\n",
    "    # Prefer current catalog, otherwise pick the first non-system catalog\n",
    "    current = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
    "    catalogs = [r.catalog for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    CATALOG = current if current not in (\"system\",) else next(c for c in catalogs if c not in (\"system\",))\n",
    "\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "VOLUME = dbutils.widgets.get(\"VOLUME\")\n",
    "BASE = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Create schema and volume**</span>\n",
    "\n",
    "Ensure the Unity Catalog schema and volume exist before loading data or saving models.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the schema and volume if needed.\n",
    "\n",
    "# Ensure schema and volume exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Load data into the UC volume**</span>\n",
    "\n",
    "Copy the diabetes CSV into the Unity Catalog volume only if it is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Avoid overwriting shared files during pipeline runs.\n",
    "\n",
    "# Sync raw data files into the UC volume (only if missing)\n",
    "data_dir = f\"{BASE}/diabetes\"\n",
    "data_file = f\"{data_dir}/diabetes.csv\"\n",
    "try:\n",
    "    dbutils.fs.ls(data_file)\n",
    "    file_exists = True\n",
    "except Exception:\n",
    "    file_exists = False\n",
    "\n",
    "if not file_exists:\n",
    "    dbutils.fs.mkdirs(data_dir)\n",
    "    dbutils.fs.cp(\"https://raw.githubusercontent.com/Ch3rry-Pi3-Azure/DataBricks-Machine-Learning/refs/heads/main/data/diabetes.csv\", data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Load, clean, and split data**</span>\n",
    "\n",
    "Read the CSV, cast columns, remove nulls, and create train/test splits for model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cast columns and split into train/test sets.\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "   \n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(BASE + \"/diabetes/diabetes.csv\")\n",
    "data = data.dropna().select(col(\"Pregnancies\").astype(\"int\"),\n",
    "                           col(\"Glucose\").astype(\"int\"),\n",
    "                          col(\"BloodPressure\").astype(\"int\"),\n",
    "                          col(\"SkinThickness\").astype(\"int\"),\n",
    "                          col(\"Insulin\").astype(\"int\"),\n",
    "                          col(\"BMI\").astype(\"float\"),\n",
    "                          col(\"DiabetesPedigreeFunction\").astype(\"float\"),\n",
    "                          col(\"Age\").astype(\"int\"),\n",
    "                          col(\"Outcome\").astype(\"int\")\n",
    "                          )\n",
    "\n",
    "   \n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Define the MLflow training function**</span>\n",
    "\n",
    "Use MLflow to track parameters and metrics while training a Spark ML pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build a pipeline, log metrics with MLflow, and save the model.\n",
    "\n",
    "def train_diabetes_model(training_data, test_data, maxIterations, regularization):\n",
    "    import mlflow\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    import time\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        numFeatures = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "        numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "        numScaler = MinMaxScaler(inputCol=numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "        featureVector = VectorAssembler(inputCols=[\"normalizedFeatures\"], outputCol=\"features\")\n",
    "        algo = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"features\", maxIter=maxIterations, regParam=regularization)\n",
    "        pipeline = Pipeline(stages=[numVector, numScaler, featureVector, algo])\n",
    "        \n",
    "        mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "        mlflow.log_param('regParam', algo.getRegParam())\n",
    "        model = pipeline.fit(training_data)\n",
    "        \n",
    "        prediction = model.transform(test_data)\n",
    "        metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "        for metric in metrics:\n",
    "            evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=metric)\n",
    "            metricValue = evaluator.evaluate(prediction)\n",
    "            print(f\"{metric}: {metricValue}\")\n",
    "            mlflow.log_metric(metric, metricValue)\n",
    "        \n",
    "        unique_model_name = \"classifier-\" + str(time.time())\n",
    "        model_path = BASE + f\"/models/{unique_model_name}\"\n",
    "        model.write().overwrite().save(model_path)\n",
    "        print(\"Experiment run complete. Model saved to\", model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Run experiment: config A**</span>\n",
    "\n",
    "Train and log a model run with a smaller iteration count and higher regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# First experiment run with chosen hyperparameters.\n",
    "\n",
    "train_diabetes_model(train, test, 5, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Run experiment: config B**</span>\n",
    "\n",
    "Train and log a model run with more iterations and lower regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Second experiment run for comparison.\n",
    "\n",
    "train_diabetes_model(train, test, 10, 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Sample request payload**</span>\n",
    "\n",
    "Example JSON payload for real?time model scoring endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "   \"dataframe_records\": [\n",
    "   {\n",
    "      \"Pregnancies\": 8,\n",
    "      \"Glucose\": 85,\n",
    "      \"BloodPressure\": 65,\n",
    "      \"SkinThickness\": 29,\n",
    "      \"Insulin\": 0,\n",
    "      \"BMI\": 26.6,\n",
    "      \"DiabetesPedigreeFunction\": 0.672,\n",
    "      \"Age\": 34\n",
    "   }\n",
    "   ]\n",
    " }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}