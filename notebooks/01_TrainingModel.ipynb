{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#1f77b4\">**Machine Learning 01 - Training Model**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Unity Catalog configuration**</span>\n",
    "\n",
    "Set up widgets for `CATALOG`, `SCHEMA`, and `VOLUME`, resolve the active catalog, and build a reusable `BASE` path for storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configure Unity Catalog widgets and resolve the active catalog.\n",
    "\n",
    "# Unity Catalog config for this project\n",
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"CATALOG\", \"\")\n",
    "dbutils.widgets.text(\"SCHEMA\", \"default\")\n",
    "dbutils.widgets.text(\"VOLUME\", \"ml_lab\")\n",
    "\n",
    "catalog_widget = dbutils.widgets.get(\"CATALOG\")\n",
    "if catalog_widget:\n",
    "    CATALOG = catalog_widget\n",
    "else:\n",
    "    # Prefer current catalog, otherwise pick the first non-system catalog\n",
    "    current = spark.sql(\"SELECT current_catalog()\").first()[0]\n",
    "    catalogs = [r.catalog for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    CATALOG = current if current not in (\"system\",) else next(c for c in catalogs if c not in (\"system\",))\n",
    "\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "VOLUME = dbutils.widgets.get(\"VOLUME\")\n",
    "BASE = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Create schema and volume**</span>\n",
    "\n",
    "Ensure the schema and volume exist in Unity Catalog so read/write operations succeed.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the schema and volume if they do not exist.\n",
    "\n",
    "# Ensure schema and volume exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Load data into the UC volume**</span>\n",
    "\n",
    "Check for the diabetes CSV and copy it from GitHub only if missing to avoid overwriting files during jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only copy the dataset when it is missing to avoid job conflicts.\n",
    "\n",
    "# Sync raw data files into the UC volume (only if missing)\n",
    "data_dir = f\"{BASE}/diabetes\"\n",
    "data_file = f\"{data_dir}/diabetes.csv\"\n",
    "try:\n",
    "    dbutils.fs.ls(data_file)\n",
    "    file_exists = True\n",
    "except Exception:\n",
    "    file_exists = False\n",
    "\n",
    "if not file_exists:\n",
    "    dbutils.fs.mkdirs(data_dir)\n",
    "    dbutils.fs.cp(\"https://raw.githubusercontent.com/Ch3rry-Pi3-Azure/DataBricks-Machine-Learning/refs/heads/main/data/diabetes.csv\", data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Preview the raw dataset**</span>\n",
    "\n",
    "Read the CSV into a Spark DataFrame and display a sample to confirm the data loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV and quickly inspect a sample of records.\n",
    "\n",
    "# Load dataset into a Spark DataFrame\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(BASE + \"/diabetes/diabetes.csv\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Clean and cast columns**</span>\n",
    "\n",
    "Drop null rows and cast each feature to the correct numeric type using `pyspark.sql.functions` so ML models can train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast fields into numeric types for ML and remove nulls.\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "   \n",
    "data = df.dropna().select(col(\"Pregnancies\").astype(\"int\"),\n",
    "                           col(\"Glucose\").astype(\"int\"),\n",
    "                          col(\"BloodPressure\").astype(\"int\"),\n",
    "                          col(\"SkinThickness\").astype(\"int\"),\n",
    "                          col(\"Insulin\").astype(\"int\"),\n",
    "                          col(\"BMI\").astype(\"float\"),\n",
    "                          col(\"DiabetesPedigreeFunction\").astype(\"float\"),\n",
    "                          col(\"Age\").astype(\"int\"),\n",
    "                          col(\"Outcome\").astype(\"int\")\n",
    "                          )\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Train/test split**</span>\n",
    "\n",
    "Split the cleaned dataset into training and testing partitions for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset for training and evaluation.\n",
    "\n",
    "# Split data into training and testing sets\n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Assemble and scale features**</span>\n",
    "\n",
    "Use `VectorAssembler` to build a feature vector and `MinMaxScaler` to normalize values for stable model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features and normalize them for model stability.\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "\n",
    "numericFeatures = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\"]\n",
    "numericColVector = VectorAssembler(inputCols=numericFeatures, outputCol = \"numericFeatures\")\n",
    "vectorizedData = numericColVector.transform(train)\n",
    "\n",
    "minMax = MinMaxScaler(inputCol= numericColVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "scaledData = minMax.fit(vectorizedData).transform(vectorizedData)\n",
    "\n",
    "compareNumerics = scaledData.select(\"numericFeatures\", \"normalizedFeatures\")\n",
    "display(compareNumerics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Prepare features and labels**</span>\n",
    "\n",
    "Select the normalized feature vector as `features` and the outcome column as `label` for Spark ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature vector and label columns expected by Spark ML.\n",
    "\n",
    "preppedData = scaledData[col(\"normalizedFeatures\").alias(\"features\"), col(\"Outcome\").alias(\"label\")]\n",
    "display(preppedData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Train logistic regression**</span>\n",
    "\n",
    "Fit a logistic regression classifier with regularization to create a baseline model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier with regularization.\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.3)\n",
    "model = lr.fit(preppedData)\n",
    "print (\"Model trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Generate predictions**</span>\n",
    "\n",
    "Transform the test data and produce predicted labels and probabilities for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data and compute predictions.\n",
    "\n",
    "# Prepare the test data\n",
    "\n",
    "vectorizedTestData = numericColVector.transform(test)\n",
    "scaledTestData = minMax.fit(vectorizedTestData).transform(vectorizedTestData)\n",
    "preppedTestData = scaledTestData[col(\"normalizedFeatures\").alias(\"features\"), col(\"Outcome\").alias(\"label\")]\n",
    "   \n",
    "# Get predictions\n",
    "prediction = model.transform(preppedTestData)\n",
    "predicted = prediction.select(\"features\", \"probability\", col(\"prediction\").astype(\"Int\"), col(\"label\").alias(\"trueLabel\"))\n",
    "display(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Evaluate the model**</span>\n",
    "\n",
    "Use `MulticlassClassificationEvaluator` to compute accuracy, precision, recall, and F1 scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute multiple classification metrics.\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "   \n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "   \n",
    "# Simple accuracy\n",
    "accuracy = evaluator.evaluate(prediction, {evaluator.metricName:\"accuracy\"})\n",
    "print(\"Accuracy:\", accuracy)\n",
    "   \n",
    "# Individual class metrics\n",
    "labels = [0,1]\n",
    "print(\"\\nIndividual class metrics:\")\n",
    "for label in sorted(labels):\n",
    "    print (\"Class %s\" % (label))\n",
    "   \n",
    "    # Precision\n",
    "    precision = evaluator.evaluate(prediction, {evaluator.metricLabel:label,\n",
    "                                                evaluator.metricName:\"precisionByLabel\"})\n",
    "    print(\"\\tPrecision:\", precision)\n",
    "   \n",
    "    # Recall\n",
    "    recall = evaluator.evaluate(prediction, {evaluator.metricLabel:label,\n",
    "                                             evaluator.metricName:\"recallByLabel\"})\n",
    "    print(\"\\tRecall:\", recall)\n",
    "   \n",
    "    # F1 score\n",
    "    f1 = evaluator.evaluate(prediction, {evaluator.metricLabel:label,\n",
    "                                         evaluator.metricName:\"fMeasureByLabel\"})\n",
    "    print(\"\\tF1 Score:\", f1)\n",
    "   \n",
    "# Weighted (overall) metrics\n",
    "overallPrecision = evaluator.evaluate(prediction, {evaluator.metricName:\"weightedPrecision\"})\n",
    "print(\"Overall Precision:\", overallPrecision)\n",
    "overallRecall = evaluator.evaluate(prediction, {evaluator.metricName:\"weightedRecall\"})\n",
    "print(\"Overall Recall:\", overallRecall)\n",
    "overallF1 = evaluator.evaluate(prediction, {evaluator.metricName:\"weightedFMeasure\"})\n",
    "print(\"Overall F1 Score:\", overallF1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Build a reusable pipeline**</span>\n",
    "\n",
    "Bundle feature engineering and the classifier into a `Pipeline` so the same steps can be reused consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to keep feature steps and model together.\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "   \n",
    "\n",
    "numFeatures = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "   \n",
    "# Define the feature engineering and model training algorithm steps\n",
    "numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "featureVector = VectorAssembler(inputCols=[\"normalizedFeatures\"], outputCol=\"Features\")\n",
    "algo = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"Features\", maxIter=10, regParam=0.3)\n",
    "   \n",
    "# Chain the steps as stages in a pipeline\n",
    "pipeline = Pipeline(stages=[ numVector, numScaler, featureVector, algo])\n",
    "   \n",
    "# Use the pipeline to prepare data and fit the model algorithm\n",
    "model = pipeline.fit(train)\n",
    "print (\"Model trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Pipeline inference**</span>\n",
    "\n",
    "Run the pipeline on test data and review predictions from the pipeline output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference using the pipeline output.\n",
    "\n",
    "prediction = model.transform(test)\n",
    "predicted = prediction.select(\"Features\", \"probability\", col(\"prediction\").astype(\"Int\"), col(\"Outcome\").alias(\"trueLabel\"))\n",
    "display(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Save the trained model**</span>\n",
    "\n",
    "Persist the pipeline model to the Unity Catalog volume so it can be loaded later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the model to UC storage for reuse.\n",
    "\n",
    "model.write().overwrite().save(BASE + \"/models/diabetes.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1f77b4\">**Load and infer with the saved model**</span>\n",
    "\n",
    "Load the saved `PipelineModel` and run inference on a new sample record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back and score a new row.\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "persistedModel = PipelineModel.load(BASE + \"/models/diabetes.model\")\n",
    "   \n",
    "newData = spark.createDataFrame ([{\"Pregnancies\": 8,\n",
    "                                  \"Glucose\": 85,\n",
    "                                  \"BloodPressure\": 65,\n",
    "                                  \"SkinThickness\": 29,\n",
    "                                  \"Insulin\": 0,\n",
    "                                  \"BMI\": 26.6,\n",
    "                                  \"DiabetesPedigreeFunction\": 0.672,\n",
    "                                  \"Age\": 34\n",
    "                                  }])\n",
    "   \n",
    "   \n",
    "predictions = persistedModel.transform(newData)\n",
    "display(predictions.select(\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\",  col(\"prediction\").alias(\"PredictedOutcome\")))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}